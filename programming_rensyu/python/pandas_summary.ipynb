{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "#環境構築\n",
    "#コマンドプロンプトやターミナルを開きます。\n",
    "#次のコマンドを入力してEnterキーを押します\n",
    "#pip install pandas\n",
    "\n",
    "#以下はワークスペース内\n",
    "#import pandas as pd\n",
    "\n",
    "#用語　Table, header, column, rowまたはレコード,  DataFrame, Series(楯も横も)\n",
    "\n",
    "#pd.Series() dictionaryを作ってそれをpd.Series()に入れるだけ\n",
    "#(np.array([])をpd.Series()に入れることもできる)\n",
    "\n",
    "#array = np.array([100, 200, 300])　labels = ['a', 'b', 'c']  \n",
    "# 上記のように定義して、pd.Series(array, labels)もできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "#DataFrameの基本的な作成方法\n",
    "#ndarrayから作る pd.DataFrame(data=ndarray, index=index, columns=columns)\n",
    "#dictionaryから作る pd.DataFrame([data1, data2, data3,...,dataN])\n",
    "#ファイルから読み込む pd.read_csv('ワークスペースと同じファイル下にあるファイル？') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12\n",
    "#(常識としてtype()は引数にいれたもののデータ形式を調べられるdftype(df['Age'])など)\n",
    "#DataFrameの変数には’df’をよく使う なんのDataFrameかを明確にしたい時は, train_df や titanic_df とするとわかりやすい\n",
    "\n",
    "#.head() で最初の５レコードを表示して内容を確認 (.head()の引数に数字をいれると，最初の「指定した数の行」のみ表示されて便利)\n",
    "#df.head()\n",
    "\n",
    "#.describe()で統計量を確認する(統計量を確認できるのは，当然数値の項目のみ)\n",
    "#df.describe()\n",
    "\n",
    "#.columnsでカラムのリストを表示する\n",
    "#df.columns\n",
    "\n",
    "#ブラケット[]で特定のカラムだけ抜き出したSeriesを取得する\n",
    "#df['Age']\n",
    "# ブラケット[]にカラムのリストを入れて複数のカラムをまとめて抽出する\n",
    "#df[['Age', 'Parch', 'Fare']].head()\n",
    "\n",
    "#.iloc[int]で特定の行をSeriesで取得する(index locationの略)\n",
    "#(df.iloc[888])\n",
    "#Seriesなので当然，df.iloc[888]['Age']のように値を取ることも可能\n",
    "#slicingで複数行取得する\n",
    "#(df = pd.read_csv('train.csv') df.iloc[5:10])\n",
    "\n",
    "\n",
    "#.drop()で特定の行，列を落とす (axis=0なら行を，axis=1ならカラムを．デフォルトはaxis=0)\n",
    "#(df.drop(['Age','Name','Ticket'], axis=1) (AgeカラムとNameカラムとTicketカラムを落とす))\n",
    "#(df.drop(3) ３行目)\n",
    "#(df.drop('Age', axis=1) (Ageカラムを落とす))\n",
    "#dropしても元のdfは変更されない．\n",
    "#元のdfを上書きしたい場合は次の２通りで上書き\n",
    "#inplace引数をTrueにする\n",
    "#df.drop(['Age', 'Cabin'], axis=1, inplace=True)\n",
    "#結果を同じdfに代入する(おすすめ)\n",
    "#df = df.drop(['Age', 'Cabin'], axis=1)\n",
    "\n",
    "\n",
    "#補足１nanとNoneの復習\n",
    "#(df.iloc[888]['Age']の戻り値はnan(Not-A-Number))\n",
    "#np.isnan(df.iloc[888]['Age'])がTrueになる\n",
    "#df.iloc[888]['Age'] is NoneがFalseになる\n",
    "\n",
    "#補足２　なんでilocなの？普通にブラケットじゃだめなの？\n",
    "#理由はブラケットはカラムの取得に使うから\n",
    "#ブラケットはカラムの取得(列のSeries)\n",
    "#iocはrow(レコード)の取得(行のSeries)\n",
    "\n",
    "\n",
    "#まとめ\n",
    "#1   DataFrameの変数にはdfをよく使う\n",
    "#2  .head()で最初の5レコードのみ表示\n",
    "#3  .describe()で統計量を確認\n",
    "#4  .columnsでカラムのリストを表示\n",
    "#5  ブラケット[]で特定のカラムだけ抜き出したSeriesを取得\n",
    "#6  .iloc[int]で特定の行をSeriesで取得\n",
    "#7  .drop()で特定の行，列を落とす\n",
    "#8  大きくなるようなデータ構造は再度同じオブジェクトに代入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13\n",
    "#フィルタ操作について\n",
    "#例えば生存者のレコードだけに絞りたい時\n",
    "#filter = df['Survived'] == 1 (filterという変数に入れて，df[]に渡す)\n",
    "#df[filter]\n",
    "# (df[filterの条件]で，ある条件に該当したレコードだけが返ってくる．Trueのところだけフィルタが通る)\n",
    "# (SQLでいうwhere句のようなもの)\n",
    "# レコードのindexは保たれ、元のDataFrameは上書きされない\n",
    "#普通はいちいち変数にはいれずdf[df['Survived']==1]とすることの方が多い\n",
    "\n",
    "\n",
    "#特定の条件を満たすレコードの統計量の確認\n",
    "#df[df['Survived']==1].describe()\n",
    "#60才以上のレコードに絞る→df[df['Age']>=60]\n",
    "#1stクラスのレコードに絞る→df[df['Pclass']==1]\n",
    "#女性のレコードに絞る→df[df['Sex']=='female']\n",
    "#\n",
    "#\n",
    "#()&() や ()|()　を複数条件でフィルタ可能\n",
    "#各条件を括弧 () で囲んで & (アンパサンド)や | (パイプ)で繋ぐ\n",
    "## 60才以上女性\n",
    "#df[(df['Age'] >= 60) & (df['Sex']=='female')]\n",
    "# 1stクラスもしくは１０歳未満\n",
    "#df[(df['Pclass'] == 1) | (df['Age'] < 10)]\n",
    "#\n",
    "#\n",
    "# ~（スクィグル）をつけるとNOT演算でフィルタ可能\n",
    "#df[~(df['Sex']=='female')]\n",
    "# ~ はBooleanのカラムで使うケースが多い\n",
    "#df[~df['Survived']]\n",
    "\n",
    "\n",
    "#indexを変更する\n",
    "#\n",
    "#フィルタを通したDataFrameのindexはもとのDataFrameのまま　→　.reset_index() をすることでindexを0から順に再度割り振\n",
    "#もとのindexは「index」というカラムになっています．(ややこし！)\n",
    "#df を更新したい場合は inplace=True もしくは df = df.reset_index() で再代入\n",
    "#\n",
    "#.set_index()で特定のカラムをindexにする(inplace=True でもとのdfを上書けます．)\n",
    "#df.set_index('Name')\n",
    "\n",
    "\n",
    "#まとめ\n",
    "#DataFrameのフィルタ操作はデータサイエンス再頻出操作の一つです．\n",
    "#df[条件のSeries]でフィルタ可能．indexはそのまま\n",
    "#()&() や ()|()　を複数条件でフィルタ可能⇨括弧を忘れずに！あとandとor出ないことに注意\n",
    "#~ （スクィグル）をつけるとNOT演算でフィルタ可能⇨とくにBooleanのカラムでよく使う\n",
    "#.reset_index()で再度indexを割り振る\n",
    "#set_index()で特定のカラムをindexにする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14 Nanの操作方法について\n",
    "\n",
    "#復習\n",
    "#NaNはNot A Numberの略\n",
    "#np.nanと同じ\n",
    "#NaN判定には np.isnan() を使う(後述: pd.isna()もあります．)\n",
    "#Noneとは別物\n",
    "#DataFrameでは基本NaNが使われる\n",
    "#csvやエクセルで値が空白だと読み込んだ時にNaNになるNoneはないことに注意\n",
    "\n",
    "\n",
    "#Nanのdrop処理\n",
    "#.dropna() NaNがある行(レコード)をdrop する（デフォルトはaxis=0で行）　\n",
    "# df.dropna()\n",
    "#axis=1を引数にいれるとNaNを含むカラムをdropできる(あまり使わないと思う)　\n",
    "# df.dropna(axis=1)\n",
    "#カラム名のリストをsubset引数に渡すことで，そのカラムにおいてNaNを含む行のみをdropしてくれる\n",
    "#(たとえ一つのカラムでも，リスト形式で渡すことに注意！)\n",
    "#df.dropna(subset=['Age'])\n",
    "#当然.dropna() しても，元の df は上書きされないから、df を更新したい場合は inplace=True か \n",
    "#df = df.dropna() で再代入\n",
    "\n",
    "\n",
    "#NaNに特定のvalueを代入\n",
    "#.fillna(value)\n",
    "#(  df.fillna('THIS IS IT!!!')  )\n",
    "#(  df['Age'] = df['Age'].fillna(df['Age'].mean())  ).fillna() はだいたいこの使い方で出てくる\n",
    "\n",
    "\n",
    "#DataFrameの中の値のNaN判定\n",
    "#(np.isnan() だといちいちループで回さないといけないし，stringsを入れるとエラーになったり，使い勝手が悪い)\n",
    "# →　pd.isna() を使う　\n",
    "#(pd.isnull() も同じだけど．最近名前が変わって pd.isna() が実装された)\n",
    "##　Cabin_nanカラムを使いして，CabinのNaN判定結果を代入する\n",
    "#df['Cabin_nan'] = pd.isna(df['Cabin'])\n",
    "#（　df[‘カラム名’] = Seriesで，そのカラムにSeriesを代入できる．）\n",
    "#（　もしdfにそのカラムがなかった場合は新たに追加する　）\n",
    "#（　基本的にこのようにしてどんどん新しいカラムを追加したり更新したりする．超頻出です．）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15 groupby（グループごと）について\n",
    "\n",
    "#.groupby().関数()で，groupbyする\n",
    "#groupbyしてグループにまとめた後に.mean()や.count()などの関数をコールする\n",
    "#df.groupby('Pclass').mean()の１とdf[df['Pclass']==1].describe().loc['mean']は同じ　　フィルタで一つ一つのグループの統計量を取っていたら大変だけど，groupbyを使えば簡単に各グループの統計量を比較することができる\n",
    "#df.groupby('Pclass').mean().loc[1]　　groubyの結果も当然DataFrameで、.loc[] で特定のグループのSeriesを取ってくることができる\n",
    "#（統計量の他にも， sum() や count() も使える）\n",
    "#df.groupby('Pclass').describe()   .groupby().describe() で，グループ別のdescribe()を一気に表示してくれる\n",
    "#df.groupby('Pclass').describe()['Age']　当然これもDataFrameだから特定のカラムのグループ別のdesribe()を確認できる\n",
    "#特にgroupby('カラム名A').describe()['カラム名B'] で，カラムAのグループ別のカラムBの統計量を見るのは重要\n",
    "# カラムを省略せずに表示\n",
    "#pd.set_option('display.max_columns', None)\n",
    "# 行を省略せずに表示\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "#.groupby() の結果はfor文で回すことができ，\n",
    "# (index, groupbyされたDataFrame)のタプルの形で回せる\n",
    "#for i, group_df in df.groupby('Pclass'):\n",
    "#    print(\"{}: group_df's type is {} and has {}\".format(i, type(group_df), len(group_df)))\n",
    "#例えば、このfor文のなかに処理を入れて、各Pclassのグループの中で，\n",
    "# 各レコードが何番目にFareが高いか数字を振ってみる\n",
    "#df = pd.read_csv('train.csv')\n",
    "#results = []\n",
    "#for i, group_df in df.groupby('Pclass'):\n",
    "#    sorted_group_df = group_df.sort_values('Fare')\n",
    "#    sorted_group_df['RankInPClass'] = np.arange(len(sorted_group_df))\n",
    "#    results.append(sorted_group_df)\n",
    "#result_df = pd.concat(results)\n",
    "#result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16 DataFrameのテーブル結合(merge, join, concat)\n",
    "\n",
    "#特定のカラムやindexをKeyにして結合する\n",
    "#.merge() 同じカラムをKey(キー)にして二つのDataFrameを横に結合\n",
    "#df1.merge(df2)\n",
    "\n",
    "#join \n",
    "#joinはmergeとほぼ同じだし、あまり使わないから無視してOK\n",
    "\n",
    "#DataFrameを単純に横に(もしくは縦に)結合する（ガッチャンコさせる）\n",
    "#.concat()（concatenateの略）第一引数にはDataFrameの入ったリスト型の変数でもできる\n",
    "# 縦  (縦につなげる方が圧倒的に多い)\n",
    "#　pd.concat([df1, df2], axis=0)  (デフォルトはaxis=0)\n",
    "# 横\n",
    "#　pd.concat([df1, df2], axis=1) \n",
    "\n",
    "\n",
    "#.merge()の詳しい使い方 (左の表に右の表をくっつけるイメージ)\n",
    "#重要な引数\n",
    "#how : どう結合するか→{‘left’, ‘right’, ‘outer’, ‘inner’}, デフォルトは ‘inner’\n",
    "#　　　left,right はhowで指定したベースのレコードのkeyと同じkeyだけ取ってくるイメージ\n",
    "#　　　（実際業務で結合する場合の多くが「大きな表に，小さな表を結合する」ケース　よく使うのは’left’と’inner’）\n",
    "# 　　　ベースの表と追加の表の過不足がないことがわかっていればinnerを使い、\n",
    "# 　　　追加の表がベースの表より足りないケースは’left’を使う\n",
    "\n",
    "# on : keyにするカラムを指定（どちらのDataFrameにも存在するカラム）．指定をしないと共通のカラムで結合される\n",
    "# 　　 指定したkey以外の共通のカラムに振られるsuffixを変更したい場合はsuffixes引数にタプルでいれればOK！\n",
    "# 　　 df1.merge(df2, on='ID', suffixes=('_left', '_right'))\n",
    "#\n",
    "#　　Keyにしたいカラム名がleftとrightで異なるとき（keyの値が同じものが対応する）\n",
    "# left_on：leftのDataFrameのkeyにするカラム\n",
    "# right_on：rightのDataFrameのkeyにするカラム\n",
    "#           df1.merge(df2, left_on='Key1', right_on='Key2')\n",
    "\n",
    "#　　カラムではなくIndexをKeyに指定したい場合(同じindexのレコード同士で結合される)\n",
    "# left_index：leftのKeyをindexにする場合Trueを指定\n",
    "# right_index：rightのKeyをindexにする場合Trueを指定\n",
    "\n",
    "#必要なときに毎回ググって出会えばいい．\n",
    "#「そういえばこういうことができる関数があったな」くらいに頭の片隅に入れておけばOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'40s'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#17 DataFrame(およびSeries)の重要な関数\n",
    "#(.unique() 　.nunique()　　.value_counts()　　,apply())\n",
    "#（.unique() 　.nunique()　　.value_counts()はSeriesの関数）\n",
    "\n",
    "#.unique() そのSeriesでユニークな値のリストを返す\n",
    "#.nunique()そのSeriesでユニークな値の数を返す\n",
    "#本当にPclassは1, 2, 3しか値がないのだろうか？　→　df['Pclass'].unique()\n",
    "\n",
    "#.value_counts()　それぞれの値に対していくつのレコードがあるのかをSeries形式で返す\n",
    "#df['Pclass'].value_counts()\n",
    "\n",
    "#.apply()  (超重要) 基本的にDataFrameの操作はapply関数で処理していく\n",
    "#(各行に処理をapplyする(適用する)イメージ)\n",
    "#\n",
    "#def get_age_group(age):\n",
    "#    return str(age)[0] + '0s'　(内容が一行だからlambda関数が使えそう...)\n",
    "#\n",
    "#df = pd.DataFrame({ 'name': ['john', 'Mike', 'Emily'],\n",
    "#                    'age': ['23', '36', '42']})\n",
    "#'age_group'カラムを新たに作り，結果を代入\n",
    "#df['age_group'] = df['age'].apply(get_age_group)\n",
    "\n",
    "#lambda関数を使った.apply()の使い方\n",
    "\n",
    "#復習\n",
    "#lambda関数を変数fに代入する\n",
    "f = lambda x: str(x)[0] + '0s' #(lambdaではよく’x’をパラメータに使う)\n",
    "#　試しに43を入れてみる\n",
    "f(43)\n",
    "\n",
    "#df['age_group'] = df['age'].apply(lambda x: str(x)[0] + '0s')\n",
    "#apply()を使う多くのケースがlambda関数で済ませれることが多い\n",
    "#df['新しいカラム名'] = df[‘カラム名’].apply(lambda x: )の形で覚えるといい（lambda関数はほとんどこの.apply()でしか使わない）\n",
    "\n",
    "#レコード全体に対して使う.apply()の使い方\n",
    "#　　(df[‘カラム名’].apply()ではSeriesの.apply()を使っている)\n",
    "#df.apply()とすることでレコード全体を引数にとることができる\n",
    "#　（行に対してapplyする場合，axis=1を指定する必要がある）\n",
    "#df = pd.DataFrame({ 'name': ['john', 'Mike', 'Emily'],\n",
    "#                    'age': ['23', '36', '42']})\n",
    "#df['description'] = df.apply(lambda row: '{} is {} years old'.format(row['name'], row['age']), axis=1)\n",
    "#　（パラメータrowには各レコードのSeriesが格納される）\n",
    "\n",
    "#まとめ\n",
    "#pd[‘カラム名’].unique()でユニークな値をNumPy Arrayで返す\n",
    "#pd[‘カラム名’].nunique()でユニークな値の数を返す\n",
    "#pd[‘カラム名’].value_counts()でそれぞれの値に対しいくつのレコードがあるかをSeriesで返す\n",
    "#pd[‘カラム名’].apply(func)で各レコードの指定したカラムの値に対して処理を行う\n",
    "#pd.apply(func, axis=1)で各レコードに対して処理を行う\n",
    "#.apply()に指定する関数が一行で書けるときlambda関数で済ます\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18 DataFrameのその他頻出関数\n",
    "#(.to_csv(),  .iterrows(),  .sort_values())\n",
    "\n",
    "#.to_csv()でDataFrameをcsv形式で保存(df.to_ファイル形式('ファイル名.拡張子'))\n",
    "#ファイル形式は基本的にcsv形式　→　ソフトに依存しないニュートラルな形式だから\n",
    "#(色々なファイル形式に対応している　excelはdf.to_excel('df_test.xlsx'))\n",
    "#to_csv() の引数にPathを渡せば指定したPathに保存される\n",
    "#index=Falseを指定するとindexを保存しないで済む（基本的にこれをやる）\n",
    "#保存先にすでに同じファイルがある場合，上書き保存されるから注意！\n",
    "#型によっては文字列で保存されてしまうことがあるから注意！(Booleanや数値は大丈夫,listはstrになってしまう)\n",
    "\n",
    "\n",
    "#.iterrows()でDataFrameをイテレーション\n",
    "#(DataFrameでは，リストのように直接for i in df:というのはできない　→　.iterrows() という関数を使う)\n",
    "#   df = pd.read_csv('train.csv')\n",
    "#   for idx, row in df.iterrows():\n",
    "#       if row['Age'] > 40 and row['Pclass'] == 3 and row['Sex'] == 'male' and row['Survived']==1:\n",
    "#           print('{} is very lucky guy...!'.format(row['Name']))\n",
    "#(各ループで(idx, row)というタプルに値が入る)\n",
    "#.apply() では，各レコードの処理をした結果を別のカラムに保存するときに使い，\n",
    "#今回の .iterows() では値を返すのではなく処理だけをしたいときに使うことが多い\n",
    "#（例えばDataFrameにファイルパスが格納されていて，\n",
    "#それを .iterrows() してファイルを移動させたり読み込んだりする）\n",
    "\n",
    "\n",
    "#.sort_values()で特定のカラムでソート\n",
    "#SeriesではなくDataFrameの関数であることに注意！(ソート対象の値が複数あるのでvalue「s」と複数形になる)\n",
    "#デフォルトは昇順ソート(小→大)で．降順ソート(大→小)にするには ascending=False を指定\n",
    "#.sort_values() はソートされたDataFrameが返される(DataFrameを更新したい場合は inplace=True を指定するか再代入)\n",
    "#df.sort_values('カラム名')\n",
    "\n",
    "#まとめ\n",
    "#df.to_csv('filepath', index=False) でDataFrameをcsvファイルで保存\n",
    "#for idx, row in df.iterrows(): でDataFrameのrowをイテレーション\n",
    "#pd.sort_values('カラム名') でソート\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19 DataFrameのその他頻出関数(pivot_table, xs)\n",
    "\n",
    "#.pivot_table()でピボットテーブルを作成\n",
    "#最初に「どのカラムを集計したいのか」を明確にしてそのカラムをvaluesに入れて,\n",
    "# あとは欲しい情報をindexとcolumnsに入れていくだけ\n",
    "#df.pivot_table(values='Price', index=['Date', 'User'], columns=['Method'])\n",
    "\n",
    "\n",
    "#.xs()でcross-section操作\n",
    "#あまり使わないが，ピボットのような複数のindexをもったDataFrameを操作する際に重宝する\n",
    "#先ほどのピボットテーブルで，例えば「Card」の行だけうまく抜き出したいときに使う\n",
    "#例えば’Jan-1’の行だけ取りたければ第12回で紹介した .loc[] で取れる\n",
    "#しかし， pivot.loc['Card']はエラーになる\n",
    "#pivot.xs('Card', level='Method')\n",
    "#levelは一番左(今回ではDate)がデフォルトで指定される(.loc[] と同じ)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
